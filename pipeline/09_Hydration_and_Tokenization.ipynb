{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Union, Tuple\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Input Filenames (Pointing to the original full-scale data)\n",
    "unified_grammar_input = \"race_specific_simplified_grammar.pickle\"\n",
    "cleaned_runner_input = \"runner_data_with_full_grammar_token_and_race_details_pared_down.pickle\"\n",
    "\n",
    "# Production Parameters\n",
    "num_splits = 100\n",
    "max_weeks_between_races = 52 * 5  # Max history to look back (5 years)\n",
    "flush_size = 5000  # Flush to disk every 5000 runners to keep RAM low\n",
    "\n",
    "# Output Dir\n",
    "splits_output_dir = \"training_splits\"\n",
    "\n",
    "@dataclass\n",
    "class RaceData:\n",
    "    race_id: str\n",
    "    distance_token: str\n",
    "    vc_conditions_token: str\n",
    "    vc_humidity_token: str\n",
    "    vc_temperature_token: str\n",
    "    vc_feels_like_token: str\n",
    "    vc_wind_speed_token: str\n",
    "    start_date_time: datetime\n",
    "\n",
    "@dataclass\n",
    "class TrainingExample:\n",
    "    unpadded_example_sequence: List[str] \n",
    "    actual_pace_seconds: int\n",
    "    raw_pace_data: List[tuple]\n",
    "\n",
    "@dataclass\n",
    "class RunnerForTraining:\n",
    "    name_gender_dedup_int: tuple\n",
    "    training_examples: List[TrainingExample]\n",
    "    split_assignment: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_split_files(runners_batch, output_dir, n_splits, base_name=\"runners_split\", compress=True):\n",
    "    \"\"\"Appends a batch of runners to their respective split files.\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Sort this batch into split buckets\n",
    "    buckets = {i: [] for i in range(n_splits)}\n",
    "    for r in runners_batch:\n",
    "        buckets[r.split_assignment].append(r)\n",
    "    \n",
    "    # Append each bucket to its file\n",
    "    for i in range(n_splits):\n",
    "        if not buckets[i]: continue\n",
    "        \n",
    "        filename = f\"{base_name}_{i:03d}.pkl\"\n",
    "        if compress: filename += \".gz\"\n",
    "        out_path = output_path / filename\n",
    "        \n",
    "        # Use 'ab' (append binary) mode\n",
    "        # Multi-pickle streams in gzipped files are valid for pickle.load() in a loop\n",
    "        with (gzip.open(out_path, \"ab\") if compress else open(out_path, \"ab\")) as f:\n",
    "            pickle.dump(buckets[i], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def pace_to_seconds(pace_str):\n",
    "    if not pace_str or \":\" not in pace_str: return None\n",
    "    try:\n",
    "        m, s = map(int, pace_str.split(\":\"))\n",
    "        return m * 60 + s\n",
    "    except: return None\n",
    "\n",
    "def convert_single_race_to_single_race_sequence(raw_race_instance):\n",
    "    rd = raw_race_instance[\"raceDetails\"]\n",
    "    return [\n",
    "        raw_race_instance[\"age_token\"],\n",
    "        raw_race_instance[\"gender_token\"],\n",
    "        rd.vc_conditions_token,\n",
    "        rd.vc_humidity_token,\n",
    "        rd.vc_temperature_token,\n",
    "        rd.vc_feels_like_token,\n",
    "        rd.vc_wind_speed_token,\n",
    "        rd.distance_token,\n",
    "        raw_race_instance[\"weeks_to_next_race_token\"],\n",
    "        raw_race_instance[\"weeks_to_final_race_token\"],\n",
    "        raw_race_instance[\"paceToken\"]\n",
    "    ]\n",
    "\n",
    "def convert_training_examples_to_dataclass(training_examples):\n",
    "    # Newest target race is at index 0 because races_inv is sorted reverse=True\n",
    "    final_race_pace = pace_to_seconds(training_examples[0][\"pace\"])\n",
    "    final_sequence = []\n",
    "    raw_pace_data_temp = []\n",
    "    \n",
    "    # reversed(training_examples) makes it Oldest -> Newest\n",
    "    for race in reversed(training_examples):\n",
    "        final_sequence.extend(convert_single_race_to_single_race_sequence(race))\n",
    "        raw_pace_data_temp.append((\n",
    "            race[\"raceDetails\"].distance_token, \n",
    "            race[\"weeks_to_final_race_token\"], \n",
    "            pace_to_seconds(race[\"pace\"])\n",
    "        ))\n",
    "    \n",
    "    return TrainingExample(\n",
    "        unpadded_example_sequence=final_sequence, \n",
    "        actual_pace_seconds=final_race_pace, \n",
    "        raw_pace_data=raw_pace_data_temp\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading input data...\")\n",
    "with open(unified_grammar_input, \"rb\") as f: race_details = pickle.load(f)\n",
    "with open(cleaned_runner_input, \"rb\") as f: runner_data = pickle.load(f)\n",
    "print(f\"Loaded {len(race_details)} races and {len(runner_data)} runners.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear and prepare output directory\n",
    "if os.path.exists(splits_output_dir):\n",
    "    shutil.rmtree(splits_output_dir)\n",
    "os.makedirs(splits_output_dir, exist_ok=True)\n",
    "\n",
    "batch_cache = []\n",
    "processed_count = 0\n",
    "total_examples_created = 0\n",
    "\n",
    "print(f\"Processing runners with streaming (batch size: {flush_size})...\")\n",
    "\n",
    "for i, (key, races) in enumerate(runner_data.items()):\n",
    "    if i % 10000 == 0: \n",
    "        print(f\"Processed {i} runners...\")\n",
    "    \n",
    "    if len(races) < 2: continue\n",
    "    \n",
    "    # Sort Newest -> Oldest\n",
    "    races_inv = sorted(races, key=lambda x: x[\"raceDetails\"].start_date_time, reverse=True)\n",
    "    \n",
    "    runner_examples = []\n",
    "    for idx in range(len(races_inv)-1):\n",
    "        candidate = [races_inv[idx].copy()]\n",
    "        candidate[0][\"weeks_to_next_race_token\"] = \"week_delta_0\"\n",
    "        candidate[0][\"weeks_to_final_race_token\"] = \"week_delta_0\"\n",
    "        \n",
    "        for j in range(idx+1, len(races_inv)):\n",
    "            curr = races_inv[j].copy()\n",
    "            dt = abs(races_inv[idx][\"raceDetails\"].start_date_time - races_inv[j][\"raceDetails\"].start_date_time)\n",
    "            dt_next = abs(races_inv[j-1][\"raceDetails\"].start_date_time - races_inv[j][\"raceDetails\"].start_date_time)\n",
    "            \n",
    "            w = int(round(dt.total_seconds() / (7 * 24 * 3600)))\n",
    "            wn = int(round(dt_next.total_seconds() / (7 * 24 * 3600)))\n",
    "            \n",
    "            curr[\"weeks_to_next_race_token\"] = f\"week_delta_{wn}\"\n",
    "            curr[\"weeks_to_final_race_token\"] = f\"week_delta_{w}\"\n",
    "            \n",
    "            if 0 < w < max_weeks_between_races: candidate.append(curr)\n",
    "            \n",
    "        if len(candidate) > 1:\n",
    "            runner_examples.append(convert_training_examples_to_dataclass(candidate))\n",
    "            \n",
    "    if runner_examples:\n",
    "        batch_cache.append(RunnerForTraining(\n",
    "            name_gender_dedup_int=key, \n",
    "            training_examples=runner_examples, \n",
    "            split_assignment=random.randint(0, num_splits - 1)\n",
    "        ))\n",
    "        processed_count += 1\n",
    "        total_examples_created += len(runner_examples)\n",
    "            \n",
    "    if len(batch_cache) >= flush_size:\n",
    "        append_to_split_files(batch_cache, splits_output_dir, num_splits)\n",
    "        batch_cache = []\n",
    "        print(f\"Flush! {processed_count} runners saved to disk so far.\")\n",
    "\n",
    "# Final flush\n",
    "if batch_cache:\n",
    "    append_to_split_files(batch_cache, splits_output_dir, num_splits)\n",
    "    \n",
    "print(f\"\\nCOMPLETED.\")\n",
    "print(f\"Total Runners with History: {processed_count}\")\n",
    "print(f\"Total Training Examples: {total_examples_created}\")\n",
    "print(f\"Files saved to: {splits_output_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
