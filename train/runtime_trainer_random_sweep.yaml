# Hyperparameter sweep specification for RunTime Transformer
search_method: random
n_trials: 50
seed: 42

parameters:
  model.d_model:
    type: categorical
    values: [256, 384, 512, 768]

  model.num_layers:
    type: int
    min: 4
    max: 12

  model.nhead:
    type: categorical
    values: [4, 8, 12, 16]

  model.dim_feedforward:
    type: categorical
    values: [1024, 2048, 3072, 4096]

  model.dropout:
    type: uniform
    min: 0.0
    max: 0.3

  training.learning_rate:
    type: log_uniform
    min: 0.00001
    max: 0.001

  training.batch_size:
    type: categorical
    values: [32, 64, 128, 256]

  training.weight_decay:
    type: log_uniform
    min: 0.000001
    max: 0.01

  training.smoothing_sigma_seconds:
    type: uniform
    min: 5.0
    max: 30.0

fixed:
  training.epochs: 15
  training.val_split: 0.2
  training.random_seed: 42
  training.use_amp: true
  logging.use_wandb: true
  logging.log_interval: 50

  # Important for sweeps / preemptible runs:
  training.checkpoint_interval_steps: 500
  training.keep_step_checkpoints: false

  # Data sources:
  # - RunTime-Full: ../pipeline/training_splits
  # - RunTime-Public: ../data/samples (update if you want to sweep on samples)
  data.splits_dir: "../pipeline/training_splits"
  data.num_files_to_load: 10
  data.pace_lookup: "../data/pace_lookup.pickle"


